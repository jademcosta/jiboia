o11y:
  #[optional] Configs related with tracing. By default, tracing is disabled at all.
  tracing:
    #[optional] Set this to true if you want to enable tracing. Right now, only the HTTP exporter is
    # used, so you need to set the env vars needed for it to find the otel-collector.
    # Read more about these env vars here:
    # https://pkg.go.dev/go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp
    # The default is `false`.
    # IMPORTANT: right now tracing is less useful because it gets only the HTTP part of the request.
    # This is intentional, to move slowly with theimplementation and avoid undesired bugs.
    enabled: true
    #[optional] The name of service which will be sent at the app.name field in spans.
    # Useful in case you have multiple deployments of Jiboia. Default is "jiboia".
    service_name: "jiboia"
  log:
    # [optional] The level of logs. Allowed values are `error`, `warn`, `info`, `debug`.
    # Default is `info`.
    level: warn
    # [optional] The format of logs. Only JSON is allowed right now, so you can forget about this field :)
    # Default is `json`.
    format: json
  profiling:
    # [optional] Set this to `true` if you want to enable the pprof
    # endpoints. Default is `false` (meaning the pprof endpoint will NOT be available).
    # This endpoint is useful for debugging and profiling the service.
    # It is available at /debug/pprof.
    enable_pprof: false

# [optional] Set this to `true` if you want to disable go max procs "auto set" feature.
# Default is `false` (meaning the automaxprocs will be turned on). Nonetheless, the lib respect
# the GOMAXPROCS env var in case it is set, so it is safe to let the default value on this config.
disable_max_procs: false

api:
  # [Optional] The port where it will serve the HTTP. Default is 9199
  port: 9099
  # [Optional] Payloadsd larger than this are denied. Use this as a protection.
  # Default is unlimited
  payload_size_limit: "1111KB"

flows:
    # The name of the flow. This will be used on logs/metrics and on the URL to intest
    # data on this flow
  - name: "my-flow"
    ingestion:
      # [Optional] When a token is defined, requests that arrive without it are denied
      token: "some-token-here"
      decompress:
        # Optional. Adding values here means the data will be decompressed before being ingested.
        # But decompression will only happen in case the correct "Content-Encoding" header is set.
        # If no header is set the data will be ingested as is.
        # Possible values are only: gzip, zlib, deflate, zstd, snappy
        active: ['gzip', 'snappy']
        # Optional. If left empty or zero, it will be unrestricted, which can cause CPU
        # starvation if it receives too many requests at the same time.
        # This is the number of concurrent decompression that can happen.
        # In case more than this number of decompressions are happening,
        # it will block and enqueue the incoming decompression requests.
        # Default is 0 (unrestricted concurrency).
        max_concurrency: 6
        # [Optional] When decompressing, this is the size of the slice used to hold the decompressed
        # data. Only change this if you really know what you are doing.
        # By setting this, you will avoid slice growing allocations.
        # Default is 512 bytes
        initial_buffer_size: 100kb
        # [Optional] Configuration for the circuit breaker that the ingestion flow uses to send itens
        # either to the accumulator (in case it is used) or to the upload queue.
        # It will avoid the overload of the service, trying to answer requests
        # (as the circuit breaker it is located at the beginning of the request)
      circuit_breaker:
        # [Optinal] The default is `false`. It is NOT recommended to disable it, as the
        # service can be overloaded with requests and waste of CPU, as the data being
        # ingested will not be enqueued (which will result in am error status code).
        # Setting it to `true` will disable the circuit breaker
        disable: false
        # [Optinal] Once the circuit breaker is opened (meaning it is not
        # accepting data), how long it will take to try to start allowing to accept calls again.
        # If you set it as zero it will be automatically set to the default, which is 100.
        open_interval_in_ms: 100
    # The amount of items in memory, waiting to be sent to storage. If this queue is full,
    # data starts to be dropped (lost)
    in_memory_queue_max_size: 1000
    # How many workers will exist to upload files, in parallel. Each worker uploads only 1
    # file at a time (before asking for more work)
    max_concurrent_uploads: 50
    # This is used on the object storage. It tells how many different dynamic prefixes it will use.
    # Increase it in case you are receiving too many rate-limit errors.
    path_prefix_count: 1
    # [Optional] This component compresses data before sending it to object storage
    compression:
      # The compression algorithm. Available types are gzip, zlib, deflate, zstd, snappy
      type: "gzip"
      # Higher numbers mean more speed and less compression
      level: "3"
      # [Optional] Set this only if you know beforehand the % of compression your payload usually
      # has. By setting this, it will prevent some "slice growing" when compression data,
      # which will result in less CPU usage. The number must be > 0 and <= 100.
      # If you set it 5, for example, it means that the size of the buffer used to write
      # the compressed data will be an array with 5% of the size of data. The resulting
      # compressed data CAN BE bigger than 5%, though. In which case it will have avoided
      # some extra slice grow allocations.
      # Avoid setting this if you are unsure, as it can ruin the performance and result in
      # "out of memory" errors.
      # Default is 2 (which means 2%)
      prealloc_slice_percentage: 2

    # [Optional] This component merges data into memory until a user defined limit, and then sends
    # it forward to be uploaded into object storage. You may use it to have bigger chunks of data.
    accumulator:
      # The size it will accumulate in memory before sending the merged data to be uploaded
      # If the value doesn't have a unit, it is considered to be bytes.
      # Allowed unities are: kb, mb, gb, tb, pb, eb
      size: 2MB
      # [Optional] The separator it will use to merge the chuncks of data
      separator: "__n__"
      # The internal queue of this component. This serves as a buffer for data yet to be merged.
      # The minimum allowed is 2 (which is a bad number, you should give it at least some room to work)
      queue_capacity: 11
      # [Optinal] Configuration for the circuit breaker the accumulator uses to enqueue items for
      # upload
      circuit_breaker:
        # [Optinal] The default is `false`. It is NOT recommended to disable it, as the
        # service can be overloaded with requests and waste of CPU, as the data being
        # ingested will not be enqueued (which will result in am error status code).
        # Setting it to `true` will disable the circuit breaker
        disable: true
        # [Optinal] Once the circuit breaker is opened (meaning it is not
        # accepting calls), how long it will take to try to accept calls again. If you set it as
        # zero it will be automatically set to the default, which is 100.
        open_interval_in_ms: 100
        # [Optional] if set, besides accumulating based on the size, it will also
        # accumulate based on the time. Setting this, means it will flush the data
        # being accumulated if the oldest item in the queue is older than this time.
        # This is useful for the cases where the flow receives
        # data in bursts, then receives small amounts of data for a while, and then bursts again
        # (in loop). It accepts only seconds as time unit.
        force_flush_after_seconds: 1
    external_queue:
      # Available values: noop, sqs. `noop` is for when you don't want to use any queue
      # (maybe because you just want to upload data into object storage)
      # Default value is `noop`.
      type: sqs
      config:
        url: "queue-url"
        region: aws-region-here
        access_key: ""
        secret_key: ""
    object_storage:
      # Available values: s3, localstorage.
      type: s3
      config:
        # How much time to wait before considering that the upload failed and canceling it.
        # Zero means no timeout.
        timeout_milliseconds: 120000 #120sec
        bucket: some-bucket-name-here
        region: aws-region-here
        endpoint: ""
        prefix: ""
        access_key: ""
        secret_key: ""
